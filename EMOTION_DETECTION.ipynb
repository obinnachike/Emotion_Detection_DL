{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK234MJCr74jAqzVTfdDpX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/obinnachike/Emotion_Detection_DL/blob/main/EMOTION_DETECTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2_3nF7APn93"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "Et1h2vsJ0l1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "bH5VkOx_032o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d muhammadhananasghar/human-emotions-datasethes"
      ],
      "metadata": {
        "id": "Zsgvl_nS1Aye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/human-emotions-datasethes.zip\" -d \"/content/dataset/\""
      ],
      "metadata": {
        "id": "tlryZwk-YqZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_directory = \"copy_the_train_path\"\n",
        "val_directory = \"copy_the_test_path\"\n",
        "CLASS_NAMES = ['angry', 'happy', 'sad']\n",
        "CONFIGURATION = {\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"IM_SIZE\": 256,\n",
        "    \"LEARNING_RATE\": 0.001,\n",
        "    \"N_EPOCHS\": 20,\n",
        "    \"DROPOUT_RATE\": 0.0,\n",
        "    \"REGULARIZATION_RATE\": 0.0,\n",
        "    \"N_FILTERS\": 6,\n",
        "    \"KERNEL_SIZE\": 3,\n",
        "    \"N_STRIDES\": 1,\n",
        "    \"POOL_SIZE\": 2,\n",
        "    \"N_DENSE_1\": 1024,\n",
        "    \"N_DENSE_2\": 128,\n",
        "    \"NUM_CLASS\": 3,\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "G90QmligZ3_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_directory,\n",
        "    labels = \"inferred\",\n",
        "    label_mode = \"categorical\"\n",
        "    class_names = CLASS_NAMES,\n",
        "    color_mode = \"rgb\",\n",
        "    batch_size = 32,\n",
        "    image_size = (256, 256)\n",
        "    shuffle = True,\n",
        "    seed = 99,\n",
        "    #validation_split = None,\n",
        "   # subset = 'trianing',\n",
        "    #interpolation = \"bilinear\",\n",
        "    #follow_links = False,\n",
        "    #crop_to_aspect_ratio = False\n",
        ")"
      ],
      "metadata": {
        "id": "BNlnyznJaM09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_directory,\n",
        "    labels = \"inferred\",\n",
        "    label_mode = \"int\"\n",
        "    class_names = CLASS_NAMES,\n",
        "    color_mode = \"rgb\",\n",
        "    batch_size = 32, or batch_size =  CONFIGURATION[\"BATCH_SIZE\"]\n",
        "    image_size = (256, 256) or image_size = (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"])\n",
        "    shuffle = True,\n",
        "    seed = 99,\n",
        "    #validation_split = None,\n",
        "   # subset = 'trianing',\n",
        "    #interpolation = \"bilinear\",\n",
        "    #follow_links = False,\n",
        "    #crop_to_aspect_ratio = False\n",
        ")"
      ],
      "metadata": {
        "id": "tN3a7y_0rzNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in val_dataset.take(1):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "DHZ7xlv0xb4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize =(12,12))\n",
        "\n",
        "for images, labels in train_dataset.take(1):\n",
        "  for i in range(16):\n",
        "    ax = plt.subplot(4,4,i+1)\n",
        "    plt.imshow(images[i]/225.)\n",
        "    plt.title(CLASS_NAMES[tf.argmax(labels[i], axis = 0).numpy()])\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "C6KIFvXVx2P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = (\n",
        "    train_dataset.prefetch(tf.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "EbafIbaV0G_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VAL_dataset = (\n",
        "    VAL_dataset.prefetch(tf.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "4E5rqqvz0bFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize_rescale_layers = tf.keras.Sequential([\n",
        "    Resizing(IM_SIZE, IM_SIZE),\n",
        "    Rescaling(1./225)\n",
        "])\n",
        "\n",
        "# remember to import rescale and resize from keras.layers"
      ],
      "metadata": {
        "id": "ipseyHyl1GjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model = tf.keras.Sequential([\n",
        "                             InputLayer(input_shape = (None, None, 3)), resize_rescale_layers,\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                             Conv2D(filters = CONFIGURATION[\"N_FILTERS\"], kernel_size = CONFIGURATION[\"KERNEL_SIZE\"], strides = CONFIGURATION[N_STRIDES], padding = 'valid', activation = 'relu',kernel_regularizer = L2(CONFIGURATION[])),\n",
        "                             BatchNormalization(),\n",
        "                             MaxPooling2D(pool_size = CONFIGURATION[POOL_SIZE], strides = CONFIGURATION[N_STRIDES]*2), Dropout(rate = CONFIGURATION[\"HP_DROPOUT\"]),\n",
        "\n",
        "                             Conv2D(filters = CONFIGURATION[\"N_FILTERS\"]*2 + 4, kernel_size = CONFIGURATION[\"KERNEL_SIZE\"], strides = CONFIGURATION[N_STRIDES], padding = 'valid', activation = 'relu',kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "                             BatchNormalization(),\n",
        "                             MaxPooling2D(pool_size = CONFIGURATION[POOL_SIZE], strides = CONFIGURATION[N_STRIDES]*2), Dropout(rate = CONFIGURATION[\"HP_DROPOUT\"]),\n",
        "\n",
        "                             Flatten(),\n",
        "                             Dense(CONFIGURATION[\"N-DENSE_1\"], activation = 'relu', kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "                             BatchNormalization(), Dropout(rate = CONFIGURATION[\"HP_DROPOUT\"]),\n",
        "\n",
        "                             Dense(CONFIGURATION[\"N-DENSE_2\"], activation = 'relu', kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "                             BatchNormalization(),\n",
        "\n",
        "                             Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = 'softmax')\n",
        "])\n",
        "\n",
        "lenet_model.summary()"
      ],
      "metadata": {
        "id": "OXyxQc--6LIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOSS FUNCTION\n",
        "loss_function = CategoricalCrossEntropy()\n",
        "#loss_function = CategoricalCrossEntropy(from_logits = True)"
      ],
      "metadata": {
        "id": "zfPCNBhzp94K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = [CategoricalAccuracy(name = \"accuracy\"),  TopKCategoricalAccuracy(k = 2, name = \"top_k_accuracy\")]"
      ],
      "metadata": {
        "id": "YY77YOLcuyc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model.compile(\n",
        "    optimizer = Adam(learning_rate = CONFIGURATION[\"LEARNING_RATE\"]),\n",
        "    loss = loss_function,\n",
        "    metrics = metrics)"
      ],
      "metadata": {
        "id": "gACzVY-KxBW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = lenet_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data = val_dataset,\n",
        "    epochs = CONFIGURATION[N_EPOCHS],\n",
        "    verbose = 1\n",
        ")"
      ],
      "metadata": {
        "id": "INtrwnhHyGyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_loss', 'val_loss'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xO7xUkjFzMWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_accuracy', 'val_accuracy'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nhGDzz6P0UeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model.evaluate(val_dataset)"
      ],
      "metadata": {
        "id": "zrdliqgo0CuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TESTING YOUR MODEL"
      ],
      "metadata": {
        "id": "SWLgoBqv0wu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = cv2.imread(\"copy the path of any single image, eg a single image of happy.jpg\")\n",
        "\n",
        "im =tf.constant(test_image, dtype = tf.float32)\n",
        "\n",
        "im = tf.expand_dims(im, axis = 0)\n",
        "\n",
        "print(CLASS_NAMES[tf.argmax(lenet_model(im), axis = -1).numpy()[0]])"
      ],
      "metadata": {
        "id": "SAzl711-0Igy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OR\n",
        "\n",
        "plt.figure(figsize =(12,12))\n",
        "\n",
        "for images, labels in val_dataset.take(1):\n",
        "  for i in range(16):\n",
        "    ax = plt.subplot(4,4,i+1)\n",
        "    plt.imshow(images[i]/225.)\n",
        "    plt.title(\"True Label - :\" + CLASS_NAMES[tf.argmax(labels[i], axis = 0).numpy()]| \"\\n\" + \"Predicted Label - :\" + CLASS_NAMES[tf.argmax(lenet_model(tf.expand_dims(images[i], axis = 0)), axis = -1).numpy()[0]])\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "38UW8juw33sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONFUSION MATRIX"
      ],
      "metadata": {
        "id": "STHTSxTrNmv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = []\n",
        "labels = []\n",
        "\n",
        "for im, label in val_dataset:\n",
        "  predicted.append(lenet_model(im))\n",
        "  labels.append(label.numpy())"
      ],
      "metadata": {
        "id": "ZAWQ0v7yNpxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.argmax(labels[:-1], axis = -1).flatten())\n",
        "print(np.argmax(predicted[:-1], axis = -1).flatten())"
      ],
      "metadata": {
        "id": "6EyyAhCuOIPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.argmax(predicted[:-1], axis = -1).flatten()\n",
        "lab = np.argmax(labels[:-1], axis = -1).flatten()"
      ],
      "metadata": {
        "id": "BMmlCewIP3V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(lab, pred)\n",
        "print(cm)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'Blues')\n",
        "\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Define a threshold if you are doing a binary classification"
      ],
      "metadata": {
        "id": "O848HLSDQE73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA AUGMENTATION"
      ],
      "metadata": {
        "id": "7hFH84Q6TbEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augment_layers = tf.keras.Sequential([\n",
        "    RandomRotation(factor = (-0.025, 0.025),),\n",
        "    RandomFlip(mode = 'horizontal',),\n",
        "    RandomContrast(factor = 0.1)\n",
        "])\n",
        "\n",
        "def augment_layer(image, label):\n",
        "  return augment_layers(image, training = True), label"
      ],
      "metadata": {
        "id": "phidRP3fTX1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = (\n",
        "    train_dataset\n",
        "    .map(augment_layer, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "    .prefetch(tf.AUTOTUNE)\n",
        ")\n",
        "\n",
        "Then retrain your model"
      ],
      "metadata": {
        "id": "okoXsi0vVaqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "USING CUTMIX"
      ],
      "metadata": {
        "id": "4-avFFBfaZG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def box(lamda):\n",
        "\n",
        "  r_x = tf.cast(tfp.distribution.Uniform(0, CONFIGURATION[\"IM_SIZE\"]).sample(1)[0], dtype = tf.int32)\n",
        "  r_y = tf.cast(tfp.distribution.Uniform(0, CONFIGURATION[\"IM_SIZE\"]).sample(1)[0], dtype = tf.int32)\n",
        "\n",
        "  r_w = tf.cast(CONFIGURATION[\"IM_SIZE\"]*tf.math.sqrt(1-lamda), dtype = tf.int32)\n",
        "  r_h = tf.cast(CONFIGURATION[\"IM_SIZE\"]*tf.math.sqrt(1-lamda), dtype = tf.int32)\n",
        "\n",
        "  r_x = tf.clip_by_value(r_x - r_w//2, 0, CONFIGURATION[\"IM_SIZE\"])\n",
        "  r_y = tf.clip_by_value(r_y - r_h//2, 0, CONFIGURATION[\"IM_SIZE\"])\n",
        "\n",
        "  x_b_r = tf.clip_by_value(r_x + r_w//2, 0, CONFIGURATION[\"IM_SIZE\"])\n",
        "  y_b_r = tf.clip_by_value(r_y + r_h//2, 0, CONFIGURATION[\"IM_SIZE\"])\n",
        "\n",
        "  r_w = y_b_r - r_x\n",
        "  if(r_w == 0):\n",
        "    r_w = 1\n",
        "\n",
        "  r_h = y_b_r - r_y\n",
        "  if(r_h == 0):\n",
        "    r_h = 1\n",
        "\n",
        "\n",
        "  return r_y, r_x, r_h, r_w"
      ],
      "metadata": {
        "id": "yjgyxx5dXtUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cutmix(train_dataset_1, trian_dataset_2):\n",
        "  (image_1, label_1), (image_2, label_2) = train_dataset_1, train_dataset_2\n",
        "\n",
        "  lamda = tfp.distribution.Beta(0.2,0.2)\n",
        "  lamda = lamda.sample(1)[0]\n",
        "\n",
        "  r_y, r_x, r_h, r_w = box(lamda)\n",
        "\n",
        "  crop_2 = tf.image.pad_to_bounding_box(image_2, r_y, r_x, r_h, r_w)\n",
        "\n",
        "  pad_2 = tf.image.pad_to_bounding_box(crop_2, r_y, r_x, CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"])\n",
        "\n",
        "  crop_1 = tf.image.crop_to_bounding_box(image = image_1,  r_y, r_x, r_h, r_w)\n",
        "\n",
        "  pad_1 = tf.image.pad_to_bounding_box(crop_2, r_y, r_x, CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"])\n",
        "\n",
        "  image = image_1 - pad_1 + pad_2\n",
        "\n",
        "  lamda = tf.cast(1 - (r_w*r_h)/(CONFIGURATION[\"IM_SIZE\"]*CONFIGURATION[\"IM_SIZE\"]), dtype = tf.float34)\n",
        "  label = lamda*tf.cast(label_1, dtype = tf.float32) + (1-lamda)*tf.cast(label_2, dtype = tf.float32)\n",
        "\n",
        "  return image, label"
      ],
      "metadata": {
        "id": "8bpqJk2qaw-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_1 = train_dataset.map(augment_layer, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset_2 = train_dataset.map(augment_layer, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "\n",
        "mixed_dataset = tf.data.Dataset.zip((train_dataset_1, train_dataset_2))"
      ],
      "metadata": {
        "id": "HIVDm5eYbFyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = (\n",
        "    mixed_dataset\n",
        "    .map(cutmix, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "    .prefetch(tf.AUTOTUNE))"
      ],
      "metadata": {
        "id": "uBPM8UAlceq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TENSORFLOW RECORDS. Helps us build more efficient models by helping us log results of our training.\n",
        "\n",
        "ADVANTAGES OF WORKING WITH TENSORFLOW RECORDS\n",
        "1. You can store your data more efficiently\n",
        "2. You can carry out preprocessing before storing the data\n",
        "3. Encourages the parallelizing of read data"
      ],
      "metadata": {
        "id": "BlSHGD4fiQa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.train import BytesList, FloatList, Int64List\n",
        "from tensorflow.train import Example, Features, Feature"
      ],
      "metadata": {
        "id": "P6c8jBdeqNIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_dataset = (\n",
        "    train_dataset\n",
        "    .unbatch\n",
        ")\n",
        "\n",
        "VAL_dataset = (\n",
        "    VAL_dataset\n",
        "    .unbatch\n",
        ")"
      ],
      "metadata": {
        "id": "UBbok0yDqsbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "3la6BXFcu01q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "id": "oYNSl6-Lu3oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_example(image, label):\n",
        "  bytes_feature = Feature(bytes_list = BytesList(value = [image]))\n",
        "\n",
        "  int_feature = Feature(\n",
        "      int64_list = Int64List(value = [label]))\n",
        "\n",
        "  example = Example(\n",
        "      features = Features(feature = {\n",
        "          'images': int_feature,\n",
        "          'labels': bytes_feature,\n",
        "      })\n",
        "  )\n",
        "  return example.SerializeToString()"
      ],
      "metadata": {
        "id": "60z9kagXu51j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a file folder on your colab files (eg tfrecords)\n",
        "\n",
        "NUM_SHARDS = 10\n",
        "PATH = 'tfrecords/shard_{:02d}.tfrecord'"
      ],
      "metadata": {
        "id": "NRUqMuqQBzJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(image, label):\n",
        "  image = tf.image.convert_image_dtype(image, dtype = tf.uint8)\n",
        "  image = tf.io.encode_jpeg(image)\n",
        "  return image, tf.argmax(label)"
      ],
      "metadata": {
        "id": "ATarJEXRthUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dataset = (\n",
        "    train_dataset\n",
        "    .map(encode_image)\n",
        ")"
      ],
      "metadata": {
        "id": "4NmZAjX9txDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for shard_number in range(NUM_SHARD):\n",
        "\n",
        "  sharded_dataset = (\n",
        "      encoded_dataset\n",
        "      .shard(NUM_SHARDS, shard_number)\n",
        "      .as_numpy_iterator()\n",
        "  )\n",
        "\n",
        "  with tf.io.TFRecordWriter(PATH.format(shard_number)) as file_writer:\n",
        "    for image, label in sharded_dataset:\n",
        "      file_writer.write(create_example(image, label))\n",
        "\n",
        "      file_writer.write(reord_bytes)"
      ],
      "metadata": {
        "id": "l-gRkVsEnmSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REUSING AND CONVERTING BACK TO TENSORFLOW DATASET"
      ],
      "metadata": {
        "id": "4kkMfnqDvdrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recons_dataset = tf.data.TFRecordDataset(\n",
        "    filenames = [PATH.format(p) for p in range(NUM_SHARDS)])"
      ],
      "metadata": {
        "id": "5-7YXNUIvk6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_tfrecords(example):\n",
        "\n",
        "  feature_description = {\n",
        "      \"images\": tf.io.FixedLenFeature([], tf.strings),\n",
        "      \"labels\": tf.io.FixedLenFeature([], tf.int64)\n",
        "  }\n",
        "\n",
        "  example = tf.io.parse_single_example(example, feature_description)\n",
        "  example[\"images\"] = tf.io.decode_jpeg(example[\"images\"], channels = 3)\n",
        "\n",
        "  return example[\"images\"], example[\"labels\"]"
      ],
      "metadata": {
        "id": "09phHBVHvu4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_dataset = (\n",
        "    recons_dataset\n",
        "    .map(parse_tfrecords)\n",
        "    .batch(CONFIGURATION[\"BATCH_SIZE\"])\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "Bwp3wRg4yGJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in parse_dataset.take(i):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "GkKf71OcyRwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use SparseCategoricalCrossEntropy() when your labels are integers (not one-hot encoded),\n",
        "#eg:\n",
        "#0    class 0\n",
        "#1    class 1\n",
        "#2    class 2\n",
        "\n",
        "#as against:\n",
        "#[1, 0, 0]   class 0\n",
        "#[0, 1, 0]   class 1\n",
        "#[0, 0, 1]   class 2\n",
        "\n",
        "\n",
        "loss_function = SparseCategoricalCrossEntropy()\n",
        "metrics = [SparseCategoricalAccuracy(name = \"accuracy\")]\n",
        "\n",
        "# then complie and train"
      ],
      "metadata": {
        "id": "beB7OpkS2fF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN ResNet"
      ],
      "metadata": {
        "id": "7tTGx3ix3oIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomConv2D(Layer):\n",
        "  def __init__(self, n_filters, kernel_size, n_strides, padding = 'valid'):\n",
        "    super(CustomConv2D, self).__init__(name = 'custom_conv2d')\n",
        "\n",
        "    self.conv = Conv2D(\n",
        "        filters = n_filters\n",
        "        kernel_size = kernel_size,\n",
        "        activation = \"relu\",\n",
        "        strides = n_strides,\n",
        "        padding = padding\n",
        "    )\n",
        "\n",
        "    self.batch_norm BatchNormalization()\n",
        "\n",
        "    def call(self, x, training = True):\n",
        "\n",
        "      x = self.conv(x)\n",
        "      x = self.batch_norm(x, training)\n",
        "\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "g2smnAya_BGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(Layer):\n",
        "  def __init__(self, n_channels, n_strides = 1):\n",
        "    super(ResidualBlock, self).__init__(name = 'res_block')\n",
        "\n",
        "    self.dotted = (n_strides != 1)\n",
        "    self.custom_conv_1 = CustomConv2D(n_channels, 3, n_strides, padding = 'same')\n",
        "     self.custom_conv_2 = CustomConv2D(n_channels, 3, 1, padding = 'same')\n",
        "\n",
        "     self.activation = Activation('relu')\n",
        "\n",
        "     if self.dotted:\n",
        "      self.custom_conv_3 = CustomConv2D(n_channels, 1, n_strides)\n",
        "\n",
        "  def call(self, input, training):\n",
        "\n",
        "    x = self.custom_conv_1(input, training)\n",
        "    x = self.custom_conv_2(x, training)\n",
        "\n",
        "    if self.dotted:\n",
        "      x_add = self.custom_conv_3(input, training)\n",
        "      x_add = Add()([x, input])\n",
        "    else:\n",
        "      x_add = Add()([x, input])\n",
        "\n",
        "    return self.activation(x_add)\n",
        "\n"
      ],
      "metadata": {
        "id": "hI0ne5Gq7Uz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet34(Model):\n",
        "  def__init__(self,):\n",
        "    super(ResNet34, self).__init__(name = 'resnet_34')\n",
        "\n",
        "    self.conv_1 = CustomConv2D(64, 7, 2, padding = 'same')\n",
        "    self.max_pool = MaxPooling2D(3,2)\n",
        "\n",
        "    self.conv_2_1 = ResidualBlock(64)\n",
        "    self.conv_2_2 = ResidualBlock(64)\n",
        "    self.conv_2_3 = ResidualBlock(64)\n",
        "\n",
        "    self.conv_3_1 = ResidualBlock(128, 2)\n",
        "    self.conv_3_2 = ResidualBlock(128)\n",
        "    self.conv_3_3 = ResidualBlock(128)\n",
        "    self.conv_3_4 = ResidualBlock(128)\n",
        "\n",
        "    self.conv_4_1 = ResidualBlock(256, 2)\n",
        "    self.conv_4_2 = ResidualBlock(256)\n",
        "    self.conv_4_3 = ResidualBlock(256)\n",
        "    self.conv_4_4 = ResidualBlock(256)\n",
        "    self.conv_4_4 = ResidualBlock(256)\n",
        "    self.conv_4_6 = ResidualBlock(256)\n",
        "\n",
        "    self.conv_5_1 = ResidualBlock(521, 2)\n",
        "    self.conv_5_2 = ResidualBlock(521)\n",
        "    self.conv_5_3 = ResidualBlock(521)\n",
        "\n",
        "    self.global_pool = GlobalAveragePooling2D()\n",
        "\n",
        "    self.fc_3 = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = 'softmax')\n",
        "\n",
        "  def call(self, x, training = True):\n",
        "    x = self.conv_1(x)\n",
        "    x = self.pool1(x)\n",
        "\n",
        "    x = self.conv_2_1(x, training)\n",
        "    x = self.conv_2_2(x, training)\n",
        "    x = self.conv_2_3(x, training)\n",
        "\n",
        "    x = self.conv_3_1(x, training)\n",
        "    x = self.conv_3_2(x, training)\n",
        "    x = self.conv_3_3(x, training)\n",
        "    x = self.conv_3_4(x, training)\n",
        "\n",
        "    x = self.conv_4_1(x, training)\n",
        "    x = self.conv_4_2(x, training)\n",
        "    x = self.conv_4_3(x, training)\n",
        "    x = self.conv_4_4(x, training)\n",
        "    x = self.conv_4_4(x, training)\n",
        "    x = self.conv_4_6(x, training)\n",
        "\n",
        "    x = self.conv_5_1(x, training)\n",
        "    x = self.conv_5_2(x, training)\n",
        "    x = self.conv_5_3(x, training)\n",
        "\n",
        "    x = self.global_pool(x)\n",
        "\n",
        "    return self.fc_3(x)\n",
        "\n",
        "\n",
        "\n",
        "    self.pool2 = MaxPooling2D(pool_size = pool_size)\n"
      ],
      "metadata": {
        "id": "AylY66gzy97T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_34 = ResNet34()\n",
        "resnet_34(tf.zeros([1,256,256,3]), training = True)\n",
        "resnet_34.summary()"
      ],
      "metadata": {
        "id": "Vz8IwhCCAqW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback = ModelCheckPoint(\n",
        "    'best_weight', monitor = \"val_accuracy\", verbose = 1, save_best_only = False, mode = 'max')"
      ],
      "metadata": {
        "id": "DMPvDc4kI4vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = [CategoricalAccuracy(name = \"accuracy\"),  TopKCategoricalAccuracy(k = 2, name = \"top_k_accuracy\")]"
      ],
      "metadata": {
        "id": "SgOZ0JQHLvze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_34.compile(\n",
        "    optimizer = Adam(learning_rate = CONFIGURATION[\"LEARNING_RATE\"]*10)\n",
        "    loss = loss_function,\n",
        "    metrics = metrics)"
      ],
      "metadata": {
        "id": "EyeETdUeLyxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION"
      ],
      "metadata": {
        "id": "93L39re7NaW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_34.load_weights('best_weights')"
      ],
      "metadata": {
        "id": "HL-hOkZmNcnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_34.evaluate(val_dataset)"
      ],
      "metadata": {
        "id": "MqzxS50TNlVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRANSFER LEARNING AND FEATURE EXTRACTION\n",
        "\n",
        "Transfer Learning can be applied in Computer Vision, Natural Langauge Processing, and Speech"
      ],
      "metadata": {
        "id": "9CjfPlS3_VLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backbone = keras.applications.EfficientNetB4(\n",
        "    include_top= False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3),\n",
        ")"
      ],
      "metadata": {
        "id": "mFKmK9l-_UKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FREEZING TO PRESERVE THE WEIGHTS OF THE FEATURE LAYERS\n",
        "\n",
        "backbone.trainable = False"
      ],
      "metadata": {
        "id": "Mb2eKLsWE2fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.Keras.Sequential([\n",
        "    Input(shape = (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3)),\n",
        "    backbone,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(CONFIGURATION['N_DENSE_1'], activation = 'relu'),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(CONFIGURATION['N_DENSE_2'], activation = 'relu')\n",
        "    Dense(CONFIGURATION['NUM_CLASSES'], activation = 'softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "1RdOj1ZQFaSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback = ModelCheckPoint(\n",
        "    'best_weight', monitor = \"val_accuracy\", verbose = 1, save_best_only = False, mode = 'max')"
      ],
      "metadata": {
        "id": "6HPdqYXvHHig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = CategoricalCrossEntropy()"
      ],
      "metadata": {
        "id": "I5Qm3FZOHZMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = [CategoricalAccuracy(name = \"accuracy\"),  TopKCategoricalAccuracy(k = 2, name = \"top_k_accuracy\")]"
      ],
      "metadata": {
        "id": "8pcWVgDhHaGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer = Adam(learning_rate = CONFIGURATION[\"LEARNING_RATE\"])\n",
        "    loss = loss_function,\n",
        "    metrics = metrics)\n",
        "\n",
        "# Then train"
      ],
      "metadata": {
        "id": "6JzHeNUDHlsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = lenet_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data = val_dataset,\n",
        "    epochs = CONFIGURATION[N_EPOCHS],\n",
        "    verbose = 1\n",
        ")"
      ],
      "metadata": {
        "id": "ythefhjhQ9Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION"
      ],
      "metadata": {
        "id": "LwNWGGAEH1lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(val_dataset)"
      ],
      "metadata": {
        "id": "wboUJtEUH3tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = cv2.imread(\"copy the path of any single image, eg a single image of happy.jpg\")\n",
        "\n",
        "test_image = cv2.resize(test_mage,(CONFIGURATION[\"LEARNING_RATE\"], CONFIGURATION[\"LEARNING_RATE\"]))\n",
        "\n",
        "im =tf.constant(test_image, dtype = tf.float32)\n",
        "\n",
        "im = tf.expand_dims(im, axis = 0)\n",
        "\n",
        "print(CLASS_NAMES[tf.argmax(model(im), axis = -1).numpy()[0]])"
      ],
      "metadata": {
        "id": "BCHAxkIrI4Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize =(12,12))\n",
        "\n",
        "for images, labels in val_dataset.take(1):\n",
        "  for i in range(16):\n",
        "    ax = plt.subplot(4,4,i+1)\n",
        "    plt.imshow(images[i]/225.)\n",
        "    plt.title(\"True Label - :\" + CLASS_NAMES[tf.argmax(labels[i], axis = 0).numpy()]| \"\\n\" + \"Predicted Label - :\" + CLASS_NAMES[tf.argmax(lenet_model(tf.expand_dims(images[i], axis = 0)), axis = -1).numpy()[0]])\n",
        "    plt.axis('off')\n",
        "\n",
        "# Then check confusion matrix"
      ],
      "metadata": {
        "id": "LFEd1V9KJsgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINETUNING TRANSFER LEARNING BUT WITH FUNCTIONAL API"
      ],
      "metadata": {
        "id": "GTYdG6eMU5qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backbone.trainable = False"
      ],
      "metadata": {
        "id": "DHmBi25NVzNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = Input(shape = (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3)),\n",
        "x = backbone(input, training = False),\n",
        "x = GlobalAveragePooling2D(x),\n",
        "x = Dense(CONFIGURATION['N_DENSE_1'], activation = 'relu')(x),\n",
        "x = BatchNormalization(x),\n",
        "\n",
        "x = Dense(CONFIGURATION['N_DENSE_2'], activation = 'relu')\n",
        "output = Dense(CONFIGURATION['NUM_CLASSES'], activation = 'softmax')(x)\n",
        "\n",
        "finetuned_model = Model(input, output)"
      ],
      "metadata": {
        "id": "gY8S2PLmVBLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model.summary()"
      ],
      "metadata": {
        "id": "p41ePCHCVmEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then start finetuning\n",
        "\n",
        "backbone.trainable = True"
      ],
      "metadata": {
        "id": "_o5kcnPvYBKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = Input(shape = (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3)),\n",
        "x = backbone(input, training = False),\n",
        "x = GlobalAveragePooling2D(x),\n",
        "x = Dense(CONFIGURATION['N_DENSE_1'], activation = 'relu')(x),\n",
        "x = BatchNormalization(x),\n",
        "\n",
        "x = Dense(CONFIGURATION['N_DENSE_2'], activation = 'relu')\n",
        "output = Dense(CONFIGURATION['NUM_CLASSES'], activation = 'softmax')(x)\n",
        "\n",
        "finetuned_model = Model(input, output)"
      ],
      "metadata": {
        "id": "xyFQ6_8dYJXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model.summary()\n",
        "\n",
        "# And complete all the other necessary steps"
      ],
      "metadata": {
        "id": "EJ0lQN3YYNzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer = Adam(learning_rate = CONFIGURATION[\"LEARNING_RATE\"]/100)\n",
        "    loss = loss_function,\n",
        "    metrics = metrics)"
      ],
      "metadata": {
        "id": "02JF5wz9Yrt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VISUALIZING INTERMEDIATE LAYERS"
      ],
      "metadata": {
        "id": "qwYks7uWfD_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_backbone = keras.applications.VGG19(\n",
        "    include_top=True,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape= (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3),\n",
        ")"
      ],
      "metadata": {
        "id": "RYO0fF7njIa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_backbone.summary()"
      ],
      "metadata": {
        "id": "58JtdGT2juhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_conv(layer_name):\n",
        "  if'conv' in layer_name:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "metadata": {
        "id": "tvv6O5EJo6Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_maps = [layer.output for layer in vgg_backbone.layer[1:], if is_conv(layer.name)]\n",
        "feature_map_model = Model(\n",
        "    inputs = vgg_backbone.input,\n",
        "    outputs = feature_maps\n",
        ")\n",
        "\n",
        "feature_map_model.summary()"
      ],
      "metadata": {
        "id": "CaNnrXsfkdDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = cv2.imread(\"copy the path of any single image, eg a single image of happy.jpg\")\n",
        "\n",
        "test_image = cv2.resize(test_mage,(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]))\n",
        "\n",
        "im =tf.constant(test_image, dtype = tf.float32)\n",
        "\n",
        "im = tf.expand_dims(im, axis = 0)\n",
        "\n",
        "f_maps = feature_map_model.predict(im)"
      ],
      "metadata": {
        "id": "N1xcJeRhmr3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(f_maps)):\n",
        "  print(f_maps[i].shape)"
      ],
      "metadata": {
        "id": "YN-AocFOoLZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINAL VISUALIZATION"
      ],
      "metadata": {
        "id": "upFVuCpvpYo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(f_maps)):\n",
        "  plt.figure(figsize = (256,256))\n",
        "  f_size = f_maps[i].shape[1]\n",
        "  n_channels = f_map[i].shape[3]\n",
        "  joint_map = np.ones((f_size, f_size*n_channels))\n",
        "\n",
        "  axs = plt.subplot(len(f_maps), 1, i+1)\n",
        "\n",
        "\n",
        "  for j in range(n_channels):\n",
        "    joint_maps[:, f_size*j:f_size*(j+1)] = f_maps[i][..., j]\n",
        "\n",
        "  plt.imshow(joint_maps[:0:512])\n",
        "  plt.axis(\"off\")\n"
      ],
      "metadata": {
        "id": "5bvaXFKtpcJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRAD-CAM METHOD"
      ],
      "metadata": {
        "id": "BlLVKDlv4Mtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model.load_weights('check the directory path')"
      ],
      "metadata": {
        "id": "ZEqyl_GSGbA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = cv2.imread(img_path)\n",
        "test_image = cv2.resize(test_image, test_image = cv2.resize(test_mage,(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"])) )\n",
        "im = tf.constant(test_image, dtype = tf.float32)\n",
        "img_array = tf.expand_dims(im, axis = 0)\n",
        "print(img_array.shape)\n"
      ],
      "metadata": {
        "id": "w_HGHFGaJPKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = pretrained_model.predict(img_array)"
      ],
      "metadata": {
        "id": "tdz6-QslKILW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.argmax(preds[0]))"
      ],
      "metadata": {
        "id": "23R_Ti4sKSwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_name = \"top_activation\"\n",
        "layer = pretrained_model.get_layer(last_conv_layer_name)\n",
        "layer_model = tf.keras.Model(pretrained_model.inputs, last_conv_layer.output)"
      ],
      "metadata": {
        "id": "PrLdoi1bMKZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_conv_layer_name.summary()"
      ],
      "metadata": {
        "id": "7U8VgTfoNPR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_layer_names = [\n",
        "    \"global_average_pooling2d\",\n",
        "    \"dense\",\n",
        "    \"dense_1\",\n",
        "    \"dense_2\",\n",
        "]"
      ],
      "metadata": {
        "id": "CP5A11CpNR8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_input = Input(shape = (8,8,2048))\n",
        "x = classifier_input\n",
        "\n",
        "for layer in classifier_layer_names:\n",
        "  x = pretrained_model.get_layer(Layer_name)(x)\n",
        "  classifier_model = Model(classifier, input, x)"
      ],
      "metadata": {
        "id": "IPVpQuNPNzVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape() as tape:\n",
        "  last_conv__layer_output = last_conv_layer_mode(img_array)\n",
        "  preds = classifier_model(last_conv_layer_output)\n",
        "  top_pred_index = tf.argmax(preds[0])\n",
        "  top_class_channel = preds[:, top_pred_index]\n",
        "\n",
        "grads = tape.gradient(top_class_channel, last_conv_layer_output)"
      ],
      "metadata": {
        "id": "1gSL2FDQn3ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grads.shape"
      ],
      "metadata": {
        "id": "LTiKCBSkpfjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT Models"
      ],
      "metadata": {
        "id": "GT_DhoCfFzdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = cv2.imread(\"copy the path of any single image, eg a single image of happy.jpg\")\n",
        "\n",
        "test_image = cv2.resize(test_mage,(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]))"
      ],
      "metadata": {
        "id": "SRWA2a5wF34P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patches = tf.image.extract_patches(\n",
        "    images = tf.expand_dims(test_image, axis = 0),\n",
        "    sizes = [1, CONFIGURATION['PATCH_SIZE'], CONFIGURATION['PATCH_SIZE'], 1],\n",
        "    strides = [1, CONFIGURATION['PATCH_SIZE'], CONFIGURATION['PATCH_SIZE'], 1],\n",
        "    rates = [1, 1, 1, 1],\n",
        "    padding = \"valid\"\n",
        ")"
      ],
      "metadata": {
        "id": "KQ5P6cvjK6Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(patches.shape)\n",
        "patches = tf.reshape(patches, (patches.shape[0], -1, 768))\n",
        "print(patches.shape)"
      ],
      "metadata": {
        "id": "JMGOUPpHYQXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figure = (8,8))\n",
        "\n",
        "for i in range(patches.shape[1]):\n",
        "  ax = plt.subplot(16, 16, i+1)\n",
        "  plt.imshow(tf.reshape(patches[0, i, :], (16, 16, 3)))\n",
        "  plt.axis(\"off\")\n"
      ],
      "metadata": {
        "id": "ULGIbOegwjP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch Encoder Layer"
      ],
      "metadata": {
        "id": "isnKmYjH1oRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEncoder(Layer):\n",
        "  def __init__(self, N_PATCHES, HIDDEN_SIZE):\n",
        "    super(PatchEncoder, self).__init__(name = 'patch_encoder')\n",
        "\n",
        "    self.liner_project = Dense(HIDDEN_SIZE)\n",
        "    self.positional_embedding = Embedding(N_PATCHES, HIDDEN_SIZE)\n",
        "    self.N_PATCHES = N_PATCHES\n",
        "\n",
        "    def call(self, x):\n",
        "      patches = tf.image.extract_patches(\n",
        "          images = x,\n",
        "          sizes = [1, CONFIGURATION['PATCH_SIZE'], CONFIGURATION['PATCH_SIZE'], 1],\n",
        "          strides = [1, CONFIGURATION['PATCH_SIZE'], CONFIGURATION['PATCH_SIZE'], 1],\n",
        "          rates = [1, 1, 1, 1],\n",
        "          padding = \"VALID\"\n",
        "      )\n",
        "      patches = tf.reshape(patches, (tf.shape(patches), 256, patches.shape[-1]))\n",
        "      embedding_input = tf.range(start = 0, limit = self.N_PATCHES, delta = 1)\n",
        "      output = self.linear_projection(patches) + self.positional_embedding = Embedding(embedding_input)\n",
        "\n",
        "      return output"
      ],
      "metadata": {
        "id": "4ncbUwuz2Km7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_enc = PatchEncoder(256, 768)\n",
        "patch_enc(tf.zeros([1, 256, 768, 3]))"
      ],
      "metadata": {
        "id": "VyxjsCW2_pqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRANSFORMER ENCODER"
      ],
      "metadata": {
        "id": "Ywvur-N7APBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(Layer):\n",
        "  def __init__(self, N_HEADS, HIDDEN_SIZE):\n",
        "    super(TransformerEncoder, self).__init__(name = 'transformer_encoder')\n",
        "\n",
        "    self.layer_norm_1 = LayerNormalization()\n",
        "    self.layer_norm_2 = LayerNormalization()\n",
        "\n",
        "    self.multi_head_att = MultiHeadAttention(N_HEADS, HIDDEN_SIZE)\n",
        "\n",
        "    self.dense_1 = Dense(HIDDEN_SIZE, activation = tf.nn.gelu)\n",
        "    self.dense_2 = Dense(HIDDEN_SIZE, activation = tf.nn.gelu)\n",
        "\n",
        "  def call(self, input):\n",
        "    x_1 = self.layer_norm_1(input)\n",
        "    x_1 = self.multi_head_att(x_1, x_1)\n",
        "\n",
        "    x_1 = Add([x_1, input])\n",
        "\n",
        "    x_2 = self.layer_norm_2(x_1)\n",
        "    x_2 = self.dense_1(x_2)\n",
        "    output = self.dense_2(x_2)\n",
        "    output = Add()([output, x_1])\n",
        "\n",
        "    patches = tf.reshape(patches, (patches.shape[0], -1, 768))\n",
        "    embedding_input = tf.range(start = 0, limit = self.N_PATCHES, delta = 1)\n",
        "    output = self.linear_projection(patches) + self.positional_embedding = Embedding(embedding_input)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "3GswgwNtAOdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_enc = TransformerEncoder(256, 768)\n",
        "patch_enc(tf.zeros([1, 256, 768, 3]))"
      ],
      "metadata": {
        "id": "RAQKPQ2NATXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BUILDING ViT Model"
      ],
      "metadata": {
        "id": "TlfXPJTGBqmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(Model):\n",
        "  def__init__(self,N_HEADS, HIDDEN_SIZE, N_PATCHES, N_LAYERS, N_DENSE_UNITS):\n",
        "    super(ViT, self).__init__(name = 'vision_transformer')\n",
        "    self.N_LAYERS = N_LAYERS\n",
        "    self.patch_encoder = PatchEncoder(N_PATCHES, HIDDEN_SIZE)\n",
        "    self.trans_encoder = [TransformerEncoder(N_HEADS, HIDDEN_SIZE) for _ in range(N_LAYERS)]\n",
        "    self.dense_1 = Dense(N_DENSE_UNITS, tf.nn.gelu)\n",
        "    self.dense_2 = Dense(N_DENSE_UNITS, tf.nn.gelu)\n",
        "    self.dense_3 = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = \"softmax\")\n",
        "\n",
        "  def call(self, input, training = True):\n",
        "\n",
        "    x = self.patch_encoder(input)\n",
        "\n",
        "    for i in range(self.N_LAYERS):\n",
        "      x = self.trans_encoders[i](x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = self.dense_1(x)\n",
        "    x = self.dense_2(x)\n",
        "\n",
        "    return self.dense_3(x)\n"
      ],
      "metadata": {
        "id": "86bQMYFYBZMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit = ViT(N_HEADS = 8, HIDDEN_SIZE = 768, N_PATCHES = 256,\n",
        "          N_LAYERS = 4, N_DENSE_UNITS = 1024)\n",
        "vit(tf.zeros([32,256,256,3]))"
      ],
      "metadata": {
        "id": "W9tOZgY1CPIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit.summary()"
      ],
      "metadata": {
        "id": "f9mcLFQcDo0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit.compile(\n",
        "    optimizer = Adam(learning_rate = CONFIGURATION[\"LEARNING_RATE\"]/100)\n",
        "    loss = loss_function,\n",
        "    metrics = metrics)"
      ],
      "metadata": {
        "id": "IgzNKoU-Enpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = vit.fit(\n",
        "    train_dataset,\n",
        "    validation_data = val_dataset,\n",
        "    epochs = CONFIGURATION[\"N_EPOCHS\"],\n",
        "    verbose = 1\n",
        ")"
      ],
      "metadata": {
        "id": "A8h7xV-qE-hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINETUNING ViT USING HUGGINGFACE"
      ],
      "metadata": {
        "id": "_e1f4u3mF5YZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "bcpyYiuBHl4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTConfig, ViTModel\n",
        "\n",
        "# Initializing a ViT vit-base-patch16-224 style configuration\n",
        "configuration = ViTConfig()\n",
        "\n",
        "# Initializing a model (with random weights) from the vit-base-patch16-224 style configuration\n",
        "model = ViTModel(configuration)\n",
        "\n",
        "# Accessing the model configuration\n",
        "configuration = model.config"
      ],
      "metadata": {
        "id": "HO1mm3zpJBV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of how to finetune\n",
        "\n",
        "from transformers import ViTConfig, ViTModel\n",
        "\n",
        "# Initializing a ViT vit-base-patch16-224 style configuration\n",
        "configuration = ViTConfig(hidden_size = 144)\n",
        "\n",
        "# Initializing a model (with random weights) from the vit-base-patch16-224 style configuration\n",
        "model = ViTModel(configuration)\n",
        "\n",
        "# Accessing the model configuration\n",
        "configuration = model.config"
      ],
      "metadata": {
        "id": "RuT-fjo9Ja-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(configuration)"
      ],
      "metadata": {
        "id": "GmZ3j66QJT2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize_rescale_hf = tf.keras.Sequential([\n",
        "    Resizing(224, 224),\n",
        "    Rescaling(1./225),\n",
        "    Permute((3,1,2))\n",
        "])"
      ],
      "metadata": {
        "id": "9SIsDKWTQROJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor, TFViTModel\n",
        "\n",
        "base_model = TFViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "inputs = Input(shape = (256, 256, 3))\n",
        "x = resize_rescale_hf(inputs)\n",
        "x = base_model.vit(x)[0][:,0,:]\n",
        "\n",
        "output = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = \"softmax\")(x)\n",
        "\n",
        "hf_model = tf.keras.Model(inputs = inputs, outputs = output)\n"
      ],
      "metadata": {
        "id": "bt0zdmqOJBKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model.compile(\n",
        "    optimizer = Adam(learning_rate = CONFIGURATION[\"LEARNING_RATE\"]/100)\n",
        "    loss = loss_function,\n",
        "    metrics = metrics)\n",
        "\n",
        "# do not use high learning rate when finetuning an already trained model( you can 5e-5)"
      ],
      "metadata": {
        "id": "uoGpordpTmr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = hf_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data = val_dataset,\n",
        "    epochs = CONFIGURATION[\"N_EPOCHS\"],\n",
        "    verbose = 1\n",
        ")"
      ],
      "metadata": {
        "id": "71420UVHTH90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING"
      ],
      "metadata": {
        "id": "fXcyBbw3QyJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = cv2.imread(\"copy the path of any single image, eg a single image of happy.jpg\")\n",
        "\n",
        "test_image = cv2.resize(test_mage,(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]))\n",
        "\n",
        "im = tf.constant(test_image, dtype = tf.float32)\n",
        "im = tf.expand_dims(im, axis = 0)"
      ],
      "metadata": {
        "id": "mgZBk9cSQ6HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model(tf.expand_dims(test_image, axis = 0))"
      ],
      "metadata": {
        "id": "qdbSImaIQ7kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model.summary()"
      ],
      "metadata": {
        "id": "BJGJwsa7SISl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0GvQkrYpwirK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WANDB"
      ],
      "metadata": {
        "id": "CbTlaXB4UKvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "8Gq8XtUdUM3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "id": "1lT6CRe2URKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ONNX"
      ],
      "metadata": {
        "id": "LxCpsxBPuTnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tf2onnx\n",
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "XCrLwXeSuVq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m tf2onnx.convert --saved-model name_of_saved_model/ --output name_of_new_model.onnx"
      ],
      "metadata": {
        "id": "1IQj9JMxuokw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WE CAN ALSO CONVERT FROM KERAS TO ONNX FORMAT"
      ],
      "metadata": {
        "id": "ol_zIwbIv6R6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model.save('vit_finetuned')"
      ],
      "metadata": {
        "id": "BJsgytbPv-e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model.save('vit_finetuned.h5')\n",
        "# saving as keras model"
      ],
      "metadata": {
        "id": "824wF6oSwXNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tf2onnx\n",
        "import onnxruntime as rt\n",
        "\n",
        "spec = (tf.TensorSpec(\n",
        "    (None, CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3),\n",
        "    tf.float32, name = \"input\"\n",
        "),)\n",
        "\n",
        "output_path = \"vit_keras.onnx\"\n",
        "\n",
        "model_proto, _ =tf2onnx.convert.from_keras(hf_model, input_signature = spec, opset = 13, out_path = output_path)\n",
        "output_names = [n.name for n in model_proto.graph.output]"
      ],
      "metadata": {
        "id": "GEp-00YawoGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_names)"
      ],
      "metadata": {
        "id": "S4AdhIlqzawR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INFERENCES"
      ],
      "metadata": {
        "id": "yu2eE0LoyWyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = cv2.imread(\"copy the path of any single image, eg a single image of happy.jpg\")\n",
        "\n",
        "test_image = cv2.resize(test_mage,(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]))\n",
        "\n",
        "im = test_image.astype(np.float32)\n",
        "im = np.expand_dims(test_image, axis = 0)"
      ],
      "metadata": {
        "id": "fHRXa6n-0Byc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "providers = [\"CPUExecutionProvider\"]\n",
        "m = rt.InferenceSession(output_path, providers = providers)\n",
        "\n",
        "t1 = time.time()\n",
        "N_PREDICTIONS = 100\n",
        "for _ in range(100):\n",
        "  onnx_pred = m.run(output_names, {\"output\": im})\n",
        "print(\"Time for a single Prediction\", time.time() - t1)/N_PREDICTIONS"
      ],
      "metadata": {
        "id": "GsF95wH8yY94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(onnx_pred)"
      ],
      "metadata": {
        "id": "IehEhJ4D0xh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHECKING FOR LATENCY"
      ],
      "metadata": {
        "id": "r-Rxc2KI1GkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "jIc4sdiy1GDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = time.time()\n",
        "hf_model(im)\n",
        "print(time.time() - t1)"
      ],
      "metadata": {
        "id": "7jYFJg5r1MpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUANTIZATION(A technique for performing computations and storing tensors at lower bit widths(lower precision representation) than the usual floating points which we have been working with, typically using 8-bit integars).\n",
        "\n",
        "1. DYNAMIC QUANTIZATION\n",
        "2. STATIC QUANTIZATION (BOTH ARE POST QUANTIZATION METHODS, MEANING AFTER TRAINING)\n",
        "3. QAT(QUANTIZATION AWARE TRAINING), this simply means that you can improve the training accuracy of quantized models if you include the quantization error in the training phase, which enables the network to adapt to the quantized weights and activations."
      ],
      "metadata": {
        "id": "ai7gOQKH4hkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUANTIZATION ON CPU"
      ],
      "metadata": {
        "id": "gmlbKJbgnjUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType"
      ],
      "metadata": {
        "id": "FWqM6NC_oK0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fp32 = '/content/drive/MyDrive/Bang/vit_keras.onnx' # saved existing model\n",
        "model_quant = '/content/vit_quantized.onnx'# specifying filepath where our quantized modle will be saved\n",
        "\n",
        "quantized_model = quantized_dynamic(model_fp32, model_quant, weight_type = QuantType.QUInt8)"
      ],
      "metadata": {
        "id": "2_ND6EHjqFz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHECKING THAT ACCURACY IS NOT HUGELY AFFECTING OR THAT IT IS MINIMAL"
      ],
      "metadata": {
        "id": "DIeM9gQJnpMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(model):\n",
        "  total, acc = 0,0\n",
        "  for im, label in validation_dataset.take(100):\n",
        "    onnx_pred = model.run(output_names, {\"input\": np.array(im)})\n",
        "\n",
        "    if (int(np.argmax(onnx_pred, axis = -1)[0][0]) = int(np.argmax(label, axis = -1[0][0]))):\n",
        "      acc += 1\n",
        "    total +=1\n",
        "return acc/total"
      ],
      "metadata": {
        "id": "7WHiybsbnz16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "providers = [\"CUDAExecutionProvider\"]\n",
        "m = rt.InferenceSession(\"output_path before quantization\", providers = providers)\n",
        "m_q = rt.InferenceSession(\"output_path after quantization\", providers = providers)\n",
        "\n",
        "print(accuracy(m))\n",
        "print(accuracy(m_q))"
      ],
      "metadata": {
        "id": "AcK2SoA2qXs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QAT(QUANTIZATION AWARE TRAINING)"
      ],
      "metadata": {
        "id": "GM6pBV_Brgoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-model-optimization"
      ],
      "metadata": {
        "id": "St3Ymjj6rh7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_quantization_to_conv(layer):\n",
        "  if \"conv\" in layer.name:\n",
        "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
        "  return layer"
      ],
      "metadata": {
        "id": "k929owS_plWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_aware_eff = tf.keras.models.clone_model(\n",
        "    pretrained_model, clone_function = apply_quantization_to_conv\n",
        ")"
      ],
      "metadata": {
        "id": "XT-upOVarH-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_aware_eff.summary()"
      ],
      "metadata": {
        "id": "NFu7HhDYr9De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_aware_model = tfmot.quantization.keras.quantize_apply(quant_aware_eff)\n",
        "quant_aware_model.summary()"
      ],
      "metadata": {
        "id": "_UWHSTOEsdEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_aware_model.compile(\n",
        "    optimizer = Adam(learning_rate = CONFIGURATION[\"LEARNING_RATE\"])\n",
        "    loss = loss_function,\n",
        "    metrics = metrics)"
      ],
      "metadata": {
        "id": "FJKNEhK8tQv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = quant_aware_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data = val_dataset,\n",
        "    epochs = CONFIGURATION[\"N_EPOCHS\"],\n",
        "    verbose = 1\n",
        ")"
      ],
      "metadata": {
        "id": "tVnMuuIxthx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUANTIZING AN ALREADY TRAINED MODEL USING TENSORFLOW LITE. (suitable for mobile and other edge devices)"
      ],
      "metadata": {
        "id": "Kk0U-d-AuIPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your trained model and evaluate on the Val dataset to  get the accuracy"
      ],
      "metadata": {
        "id": "haLOYkWMwIA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def representative_data_gen():\n",
        "  for input_value, j in training_dataset.take(20):\n",
        "    yield [input_value]"
      ],
      "metadata": {
        "id": "JgIvcV3P1SVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(name_of_your_model)\n",
        "converter.optimizations = [tf.lite.Optimize.Default]\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "converter.representative_dataset = representativae_data_gen\n",
        "\n",
        "# do not specify representativae_data_gen if you are performing a dynamic quantization"
      ],
      "metadata": {
        "id": "Ub3GYUkH0NGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tflite_model = converter.convert()"
      ],
      "metadata": {
        "id": "KN5-Ibbt0uK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAVE IN TFLITE FORMAT"
      ],
      "metadata": {
        "id": "x32uqx0h2G2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "tflite_models_dir = pathlib.Path(\"file_path\")\n",
        "tflite_models_dir.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "tflite_model_file = tflite_models_dir/'eff_model.tflite'\n",
        "tflite_model_file.write_bytes(tflite_model)\n",
        "\n"
      ],
      "metadata": {
        "id": "sRq9SpES2Ete"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALL TFLITE RUNTIME"
      ],
      "metadata": {
        "id": "74ooKbl44RcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime"
      ],
      "metadata": {
        "id": "d0MPl9Cg3ywq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tflite_runtime as tflite\n",
        "import numpy as np\n",
        "import cv2"
      ],
      "metadata": {
        "id": "KJYtyIIj4ZYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = cv2.imread(\"copy the file path of any single image, eg a single image of happy.jpg\")\n",
        "\n",
        "test_image = cv2.resize(test_mage,(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]))\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "\n",
        "#print(CONFIGURATION['CLASS_NAMES'][tf.argmax(pretrained_model(im), axis = -1).numpy()[0]])"
      ],
      "metadata": {
        "id": "i0rvWf5q43ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_path = \"copy the file path of eff_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()[0]\n",
        "output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "#test_image = im.numpy().astype(input_details[\"dtype\"])\n",
        "interpreter.set_tensor(input_details[\"index\"], test_image)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "output = interpreter.get_tensor(output_details[\"index\"])[0]"
      ],
      "metadata": {
        "id": "tYnqMP8S69s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(CONFIGURATION['CLASS_NAMES'][np.argmax(output)])"
      ],
      "metadata": {
        "id": "geBIYw9T-d0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(model_path):\n",
        "  total, correct = 0,0\n",
        "  interpreter = tf.lite.Interprater(model_path = \"copy the file path of eff_model.tflite\")\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  for im, label in validation_dataset.take:\n",
        "    test_image = im.numpy().astype(input_details[\"dtype\"])\n",
        "\n",
        "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
        "\n",
        "    interpreter.invoke()\n",
        "\n",
        "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "\n",
        "    if(int(np.argmax(output)) == int(np.argmax(label, axis = -1)[0])):\n",
        "      correct += 1\n",
        "    total += 1\n",
        "  return correct/total"
      ],
      "metadata": {
        "id": "44jqaYNg_wKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(\"copy the file path of eff_model.tflite\")"
      ],
      "metadata": {
        "id": "Vv_MBhADCOpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "APIs\n",
        "\n",
        "signup on web.postman.co and click on workplace. locat the \"GET\" (which is a drop down) and the URL beside it.\n",
        "\n",
        "The \"GET\" is used to retrieve data\n",
        "The \"POST\" is used to submit an entity to the specific resourse\n",
        "The \"PUT\" is used to update"
      ],
      "metadata": {
        "id": "l2zWaI1mEKXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BUILDING APIs USING FASTAPI"
      ],
      "metadata": {
        "id": "Y5l-0evzJrJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "use the anaconda"
      ],
      "metadata": {
        "id": "SLcznspCKcfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"fastapi[all]\""
      ],
      "metadata": {
        "id": "MU_BXt-OEJdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fastapi"
      ],
      "metadata": {
        "id": "eYg_gHvvKlfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fastapi.__version__"
      ],
      "metadata": {
        "id": "Go9U-k8VK8cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEALING WITH FASTAPIs OF DIFFERENT VERSIONS AND DIFFERENT MODELS"
      ],
      "metadata": {
        "id": "8V4wf-JVO8cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apt install python3-venv\n",
        "#then cd into your model directory and create a virtual environment"
      ],
      "metadata": {
        "id": "EM5HGqxePa2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#virtual environment\n",
        "\n",
        "python3 -m venv venv-emotions-detection"
      ],
      "metadata": {
        "id": "3gaCMa8VQrru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd venv-emotions-detection/"
      ],
      "metadata": {
        "id": "jxcUyz6tQ_OA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}